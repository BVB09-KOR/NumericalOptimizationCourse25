{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d593903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all = [var for var in globals() if var[0] != '_']\n",
    "for var in all:\n",
    "    del globals()[var]\n",
    "del var, all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fac2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from scipy.optimize import minimize\n",
    "import jax\n",
    "import jax.numpy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c941aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax 내부 연산이 float64 데이터 타입 기반으로 작동되게 설정 ... 원래 jax 내부 연산 시 default 데이터타입은 float32임.\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3828766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function\n",
    "def objective(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "# Gradient of the objective\n",
    "def objective_grad(x):\n",
    "    return numpy.array([2*x[0], 2*x[1]])\n",
    "\n",
    "# Constraint: x^2 + y^2 - 1 = 0\n",
    "def constraint(x):\n",
    "    return x[0]**2 + x[1]**2 - 1\n",
    "\n",
    "# Gradient of the constraint\n",
    "def constraint_grad(x):\n",
    "    return numpy.array([2*x[0], 2*x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a77e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients using JAX\n",
    "objective_grad_j = jax.grad(objective)\n",
    "constraint_grad_j = jax.grad(constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cddcde69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JAX arrays to numPy for scipy\n",
    "def objective_grad_jax(x):\n",
    "    return numpy.array(objective_grad_j(x)).astype(numpy.float64)\n",
    "\n",
    "def constraint_grad_jax(x):\n",
    "    return numpy.array(constraint_grad_j(x)).astype(numpy.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bb66667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial guess\n",
    "x0 = numpy.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4ae768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble Constraint\n",
    "cons = {'type': 'eq', 'fun': constraint}\n",
    "cons_withgrad = {'type': 'eq', 'fun': constraint, 'jac': constraint_grad}\n",
    "cons_withgrad_jax = {'type': 'eq', 'fun': constraint, 'jac': constraint_grad_jax}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize using SLSQP\n",
    "minimize_function1 = minimize(objective, x0, method='SLSQP', constraints=[cons]) # scipy.optimize 모듈은 사용자가 gradient를 명시적으로 주지 않을 시 obj func을 기반으로 gradient를 FD로 계산.\n",
    "minimize_function2 = minimize(objective, x0, method='SLSQP', jac=objective_grad, constraints=[cons_withgrad]) # Analytical Gradient를 사용하여 optimization\n",
    "minimize_function3 = minimize(objective, x0, method='SLSQP', jac=objective_grad_jax, constraints=[cons_withgrad_jax]) # AD를 사용하여 optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution (x, y):                               [0.70707035 0.70707035]\n",
      "Optimal solution with Analytical Gradient(x, y):       [0.70707733 0.70707733]\n",
      "Optimal solution with Automatic Differentiation(x, y): [0.70707733 0.70707733]\n"
     ]
    }
   ],
   "source": [
    "# Output - Analytical Gradient를 써서 구한 x*와 AD를 써서 구한 x*가 같은 것을 볼 수 있다.\n",
    "print(\"Optimal solution (x, y):                              \", minimize_function1.x)\n",
    "print(\"Optimal solution with Analytical Gradient(x, y):      \", minimize_function2.x) # Analytical Gradient 써서 구한 x*\n",
    "print(\"Optimal solution with Automatic Differentiation(x, y):\", minimize_function3.x) # AD 써서 구한 x*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (general_purpose)",
   "language": "python",
   "name": "general_purpose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
